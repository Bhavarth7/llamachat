{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34046,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":28500}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nimport os\n\n\nmodel_path = \"/kaggle/input/llama-3/transformers/70b-chat-hf/1/llama3-70b-chat-hf\"\n\nif not os.path.exists(model_path):\n    raise ValueError(f\"The path {model_path} does not exist. Please check the directory and try again.\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-16T18:41:59.662405Z","iopub.execute_input":"2024-06-16T18:41:59.668050Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3d9fe94cf294d7ab5237713e198fdd0"}},"metadata":{}}]},{"cell_type":"code","source":"def chat_with_llama3():\n    print(\"Chat with LLaMA-3 70B. Type 'exit' to quit.\")\n    \n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == 'exit':\n            print(\"Goodbye!\")\n            break\n\n        # Encode the user input and generate a response\n        inputs = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\"pt\")\n        outputs = model.generate(inputs, max_length=500, pad_token_id=tokenizer.eos_token_id)\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        print(f\"LLaMA-3 70B: {response}\")\n\nif __name__ == \"__main__\":\n    chat_with_llama3()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-16T18:45:54.233926Z","iopub.execute_input":"2024-06-16T18:45:54.235356Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Chat with LLaMA-3 70B. Type 'exit' to quit.\n","output_type":"stream"}]}]}