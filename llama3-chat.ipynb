{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34046,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":28500}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nimport os\n\n# Path to your model files\nmodel_path = \"/kaggle/input/llama-3/transformers/70b-chat-hf/1/llama3-70b-chat-hf\"\n\n# Ensure the path exists\nif not os.path.exists(model_path):\n    raise ValueError(f\"The path {model_path} does not exist. Please check the directory and try again.\")\n\n# Load the pre-trained LLaMA-3 70B model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-16T18:29:27.995848Z","iopub.execute_input":"2024-06-16T18:29:27.996272Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bea56de348f643f6acba2779c57dc220"}},"metadata":{}}]},{"cell_type":"code","source":"def chat_with_llama3():\n    print(\"Chat with LLaMA-3 70B. Type 'exit' to quit.\")\n    \n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == 'exit':\n            print(\"Goodbye!\")\n            break\n\n        # Encode the user input and generate a response\n        inputs = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\"pt\")\n        outputs = model.generate(inputs, max_length=500, pad_token_id=tokenizer.eos_token_id)\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        print(f\"LLaMA-3 70B: {response}\")\n\nif __name__ == \"__main__\":\n    chat_with_llama3()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-16T18:32:29.565577Z","iopub.execute_input":"2024-06-16T18:32:29.566145Z","iopub.status.idle":"2024-06-16T18:32:33.004709Z","shell.execute_reply.started":"2024-06-16T18:32:29.566099Z","shell.execute_reply":"2024-06-16T18:32:33.001878Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Chat with LLaMA-3 70B. Type 'exit' to quit.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  HI\n"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLaMA-3 70B: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mchat_with_llama3\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[1], line 11\u001b[0m, in \u001b[0;36mchat_with_llama3\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Encode the user input and generate a response\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mencode(user_input \u001b[38;5;241m+\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(inputs, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id)\n\u001b[1;32m     13\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"],"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error"}]}]}